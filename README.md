# Voice Navigator Project

## Results:

<p align="left">
  <img src="https://github.com/anshuman03sinha/Voice-Navigator/blob/master/static/VoiceNavigator.jpg" width="400" height="750" alt="Anshuman_VoiceNavigator_Result">
</p>

_Objects in increasing order of distance from user are cell phone, and person._

## Pre-trained Models
* [YoloV3 Object Detection](https://drive.google.com/file/d/1ant86ZMYAbkm13x4nxQNYoVW1tK2bLJA/view?usp=sharing) (236.5 MB)
* [NYU Depth V2](https://drive.google.com/file/d/1ant86ZMYAbkm13x4nxQNYoVW1tK2bLJA/view?usp=sharing) (164.9 MB)

## Execution
* `git clone https://github.com/anshuman03sinha/Voice-Navigator` from a python enabled terminal/editor.
* `pip install -r requirements txt` to install all necessary packages.
* `python app.py` to run the final app.
* The basic Flask app will run on the users' localhost port (**webcam access necessary**).

## On The Web
* [Presentation](https://docs.google.com/presentation/d/1-MM_Hfxjozg8s0kwolBSrW3YGDCsHIrH03xPskpl9Fs/edit?usp=sharing)
* [Demo](https://www.linkedin.com/posts/istenitk_the-aim-of-iste-crypt-members-for-this-project-ugcPost-6789912917052653569-UWkV)

## Cited
```
@article{Alhashim2018,
  author    = {Ibraheem Alhashim and Peter Wonka},
  title     = {High Quality Monocular Depth Estimation via Transfer Learning},
  journal   = {arXiv e-prints},
  volume    = {abs/1812.11941},
  year      = {2018},
  url       = {https://arxiv.org/abs/1812.11941},
  eid       = {arXiv:1812.11941},
  eprint    = {1812.11941}
}
```
